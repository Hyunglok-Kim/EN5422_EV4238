{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework #6: Trees and Random Forest\n",
        "\n",
        "# Problem 1: Tree Splitting for classification\n",
        "\n",
        "Consider the Gini index, classification error, and entropy impurity measures in a simple classification setting with two classes.\n",
        "\n",
        "Create a single plot that displays each of these quantities as a function of $p_m$, the estimated probability of an observation in node $m$ being from class 1. The x-axis should display $p_m$, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy."
      ],
      "metadata": {
        "id": "Hjn8DDEte4Fe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M5wcVCudzBF"
      },
      "outputs": [],
      "source": [
        "# Add solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Combining bootstrap estimates\n",
        "\n",
        "Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of $X$, produce the following 10 estimates of $\\Pr(\\text{Class is Red} \\mid X=x)$: {0.2, 0.25, 0.3, 0.4, 0.4, 0.45, 0.7, 0.85, 0.9, 0.9}.\n",
        "\n",
        "## a. Majority Vote\n",
        "\n",
        "ISL 8.2 describes the *majority vote* approach for making a hard classification from a set of bagged classifiers. What is the final classification for this example using majority voting?\n"
      ],
      "metadata": {
        "id": "lWjDuawHfbNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add solution here"
      ],
      "metadata": {
        "id": "fr4smpHIf-pC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## b. Average Probability\n",
        "\n",
        "An alternative is to base the final classification on the average probability. What is the final classification for this example using average probability?"
      ],
      "metadata": {
        "id": "pz1LKa-zgCci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add solution here"
      ],
      "metadata": {
        "id": "2BgVH6bigFll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Unequal misclassification costs\n",
        "Suppose the cost of mis-classifying a Red observation (as Green) is twice as costly as mis-classifying a Green observation (as Red). How would you modify both approaches to make better final classifications under these unequal costs? Report the final classifications."
      ],
      "metadata": {
        "id": "A3T_7aAzgI9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add solution here"
      ],
      "metadata": {
        "id": "mcWb-_eZgLad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Random Forest Tuning\n",
        "\n",
        "Random Forest has several tuning parameters that you will explore in this problem. We will use a Ames housing dataset.\n",
        "Please refer the following link to laod the ames_housing data:\n",
        "[Ames data here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_ames_housing.html)\n",
        "\n",
        "In Python, the `RandomForestRegressor` from the `sklearn.ensemble` module is a popular implementation for regression problems.\n",
        "\n",
        "## a. Random Forest (`RandomForestRegressor`) tuning parameters\n",
        "\n",
        "List all of the Random Forest tuning parameters in the `RandomForestRegressor` class from the `sklearn.ensemble` module. Exclude the parameters related to computation (like `n_jobs`), special models, or anything that won't impact the predictive performance.\n",
        "\n",
        "Indicate the tuning parameters you think will be most important to optimize."
      ],
      "metadata": {
        "id": "jnmxF7C_gN-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add solution here"
      ],
      "metadata": {
        "id": "Ue8qQh9Zi-Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Implement Random Forest\n",
        "\n",
        "Use a random forest model to predict the sales price, `SalePrice`. Use the default parameters and report the 10-fold cross-validation RMSE (square root of mean squared error).\n"
      ],
      "metadata": {
        "id": "Z6B5ELyOjAjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add solution here"
      ],
      "metadata": {
        "id": "fQzf9cfPjGnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Random Forest Tuning\n",
        "\n",
        "Now we will vary the tuning parameters of `max_features` and `min_samples_leaf` to see what effect they have on performance.\n",
        "\n",
        "- Use a range of reasonable `max_features` and `min_samples_leaf` values.\n",
        "    - The valid `max_features` values are `{1, 2, ..., p}` where `p` is the number of predictor variables. However, the default value of `max_features` is usually close to optimal, so you may want to focus your search around those values.\n",
        "    - The default `min_samples_leaf=1` will grow deep trees. This might work best if there are enough trees. However, try some larger values and see how it impacts predictive performance.\n",
        "    - Set `n_estimators=1000`, which is larger than the default of 100.\n",
        "- Use 5 times repeated cross-validation to assess performance instead of out-of-bag (OOB) since scikit-learn does not use OOB by default for regression.\n",
        "- Use a plot to show the average MSE as a function of `max_features` and `min_samples_leaf`.\n",
        "- Report the best tuning parameter combination.\n",
        "- Note: Random forest is a stochastic model; it will be different every time it runs. Set the random seed via the `random_state` parameter to control the uncertainty associated with the stochasticity.\n",
        "- Hint: In scikit-learn, you can use the `mean_squared_error` from `sklearn.metrics` to calculate MSE for your cross-validation results.\n"
      ],
      "metadata": {
        "id": "llL1d17rjH0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add solution here"
      ],
      "metadata": {
        "id": "bYWCIJSymEM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}